Here are three examples seperated by ### marks:

###
Problem: The performance of existing CPU- and GPU-based Embedding-based Retrieval (EBR) systems for recommendation systems is suboptimal due to their inherent architectural limitations. A good EBR system should achieve both high throughput and low latency, which is crucial for cost savings and user experience improvement.

Observations: 1. CPU-based EBR systems suffer from low memory bandwidth and limited cores, hindering simultaneous support for desired parallelism paradigms and batch queries. 2. GPU-based EBR systems provide higher memory bandwidth and compute cores, but are not optimized for pipeline parallelism, leading to increased latency from explicit kernel boundaries and suboptimal inter-operator communication. 3. K-selection for GPU-based systems either heavily relies on on-chip memory with computation overheads or uses external memory, exacerbating latency with frequent memory accesses. 4. FPGA has desired properties ideal for EBR: large high-bandwidth memory (HBM), sufficient on-chip memories, and programmable compute elements supporting customized parallelism, making it a viable platform for optimizing EBR performance.

Solution: 1. [Batching] Batch queries are executed together to share the cost of scanning the corpus among all queries in the batch. This means the system only scans the corpus once per batch, and simultaneous compute pipelines process different queries, achieving linear throughput scalability with the batch size while preserving low query latency.  2. [Precomputing] FAERY introduces a filter in the pipeline that can early drop the items that cannot make it into the Top-K, instead of waiting until the end of the pipeline. By detecting and discarding these non-competitive scores before K-selection, FAERY reduces the data volume processed in later stages, saving both computation and on-chip resources.  3. [Hardware] FAERY stores the entire item embedding corpus in the FPGAâ€™s high-bandwidth memory (HBM), which provides much higher bandwidth than traditional CPU DRAM or GPU memory. It exploits HBMâ€™s multi-channel parallelism and uses fully pipelined, streaming operator designs on the FPGA fabric for similarity computation and K-selection. The similarity calculation is parallelized to match HBM bandwidth, and a single, efficient K-selection pipeline is used thanks to early filtering. These hardware-accelerated pipelines achieve near-optimal latency and can scale linearly with batch size, outperforming CPU and GPU-based approaches in both latency-bounded throughput and resource efficiency.


###
Problem: The paper addresses the challenge of efficiently scheduling tasks in large-scale data analytics frameworks. These frameworks are trending towards shorter task durations and greater parallelism to achieve low latency. The problem is how to implement a task scheduler capable of managing millions of scheduling decisions per second while ensuring millisecond-level latency and high availability in distributed systems.

Observations: 1. The demand for low latency interactive data processing has led to the creation of frameworks that distribute work across thousands of machines.  2. Running sub-second tasks requires a scheduler capable of much higher throughput than existing centralized solutions.  3. Failure of a centralized scheduler involves complex recovery processes because large amounts of state need to be replicated or recovered quickly.  4. Probing multiple schedulers can result in conflicting actions due to delays and race conditions.  5. The ""power of two choices"" load balancing technique used naively doesn't perform well for parallel jobs due to a focus on individual task wait times.  6. Centralized schedulers struggle to handle the needs of decentralized, highly parallel workloads due to inherent throughput and availability limitations.

Solution: 1. [Deferring] Late binding is implemented to delay the assignment of tasks to worker machines until the machines are ready to execute the tasks. This approach minimizes median job response time by avoiding premature task placement and mitigating the impact of queue length mispredictions on scheduling decisions. 2. [Relaxation] The system leverages a variant of the power of two choices load balancing technique, wherein multiple servers are probed randomly, and tasks are assigned to the server with fewer queued tasks. This strategic adaptation allows tasks to be distributed efficiently without comprehensive system state awareness.


###
Problem: Constructing services that provide both low latency and high throughput is challenging. Traditional thread management operated by operating systems often results in resource underutilization or suboptimal performance due to lack of core awareness, leading to difficulties in adjusting thread parallelism according to available core resources.

Observations: 1. Traditional threading models, such as those used by the operating system, provide no visibility into the physical cores allocated to applications, which prevents applications from efficiently managing parallelism.  2. Techniques for achieving low latency, like reserving cores and using polling instead of interrupts, can result in resource wastage.  3. Combining low-latency and high-throughput services is challenging, often because techniques designed to minimize latency result in underutilized resources, especially in multi-level services with nested requests.  4. Static assignment of threads to cores often results in inefficient load-balancing and performance degradation during periods of low load or high load imbalance.  5. Background tasks, like garbage collection, can interfere with request servicing unless additional resources are reserved.  6. Ideally, applications should be able to match their workload with available cores dynamically, but traditional systems do not allow this.

Solution: 1. [Caching] Each thread context is bound to a core, and recently-used thread contexts are reused when creating new threads. This minimizes cache misses during thread creation, as thread context data (stacks and metadata) are likely to reside in the relevant coreâ€™s cache, enabling the rapid creation of user threads that are suitable for extremely short workloads. 2. [Contextualization] The user space thread implementation allows applications to communicate their resource (core) needs to the scheduler at runtime, and to compute requirements dynamically based on their workload. Each application selects a core policy to decide, with live performance statistics, how many cores it needs and how threads are placed across those cores. In Arachne, instead of relying on ready queues for scheduling, each dispatcher repeatedly scans all active user thread contexts on its associated core and chooses one that is runnable. This scan is efficient because there are typically only a few contexts per core, and the inevitable cache miss for waking a runnable thread can be overlapped with the scanning process, amortizing the cost. This approach enables applications to leverage runtime knowledge for optimal placement and scheduling decisions, achieving both low latency and high throughput on modern multicore hardware.

###


